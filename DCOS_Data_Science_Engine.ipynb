{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PaaS to Accelerate Data Science - DC/OS Data Science Engine\n",
    "\n",
    "This is part of a series of posts on using PaaS to accelerate enterprise data science teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from pyarrow.parquet import ParquetDataset\n",
    "import s3fs\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fixes display issue\n",
    "pd.set_option('display.html.border', 0)\n",
    "\n",
    "# Set the bucket that holds our data\n",
    "BUCKET = 'stackoverflow-events'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always use a random seed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test GPU Support for Tensorflow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_avail = tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")\n",
    "print(f'1 or more GPUs is available: {gpu_avail}')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "local_devices = device_lib.list_local_devices()\n",
    "gpu = local_devices[3]\n",
    "print(f\"{gpu.name} is a {gpu.device_type} with {gpu.memory_limit / 1024 / 1024 / 1024:.2f}GB RAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Stackoverflow Questions\n",
    "\n",
    "For the moment, `pandas.read_parquet` doesn't support loading directories on S3.\n",
    "See: [pandas/issues/28490](https://github.com/pandas-dev/pandas/issues/28490)\n",
    "\n",
    "Because the questions are loading from S3, this can take a few minutes, especially outside of AWS. Inside AWS it shouldn't take very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Stack Overflow questions right from S3\n",
    "s3_parquet_path = f's3://{BUCKET}/08-05-2019/Questions.Stratified.Final.2000.parquet'\n",
    "s3_fs = s3fs.S3FileSystem()\n",
    "\n",
    "# Use pyarrow.parquet.ParquetDataset and convert to pandas.DataFrame\n",
    "posts_df = ParquetDataset(\n",
    "    s3_parquet_path,\n",
    "    filesystem=s3_fs,\n",
    ").read().to_pandas()\n",
    "\n",
    "# Trim the data in half for a p3.2xlarge\n",
    "posts_df = posts_df[:500000]\n",
    "\n",
    "posts_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '{:,} Stackoverflow questions with a tag having at least 2,000 occurrences'.format(\n",
    "        len(posts_df.index)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Precomputed Indexes for Converting Between Tag Indexes and Tags\n",
    "\n",
    "We use these to display the actual labels predicted at the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tag indexes\n",
    "s3_client = boto3.resource('s3')\n",
    "\n",
    "def json_from_s3(bucket, key):\n",
    "    \"\"\"Given a bucket and key for a JSON object, return the parsed object\"\"\"\n",
    "    obj = s3_client.Object(bucket, key)\n",
    "    obj.get()['Body'].read().decode('utf-8')\n",
    "    json_obj = json.loads(obj.get()['Body'].read().decode('utf-8'))\n",
    "    return json_obj\n",
    "\n",
    "\n",
    "tag_index = json_from_s3(BUCKET, '08-05-2019/tag_index.2000.json')\n",
    "index_tag = json_from_s3(BUCKET, '08-05-2019/index_tag.2000.json')\n",
    "\n",
    "list(tag_index.items())[0:5], list(index_tag.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check the different files\n",
    "assert( len(tag_index.keys()) == len(index_tag.keys()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim the Data into a Multiple of Batch Sizes\n",
    "\n",
    "This is a requirement for Tensorflow/Keras to split work among multiple GPUs. We also join the previously tokenized text into a single string to use `tf.keras.preprocessing.text.Tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 200\n",
    "TOKEN_COUNT = 10000\n",
    "EMBED_SIZE = 50\n",
    "TEST_SPLIT = 0.3\n",
    "\n",
    "# Convert label columns to numpy array\n",
    "labels = posts_df[list(posts_df.columns)[1:]].to_numpy()\n",
    "\n",
    "# Training_count must be a multiple of the BATCH_SIZE times the MAX_LEN for the Elmo embedding layer\n",
    "highest_factor = math.floor(len(posts_df.index) / (BATCH_SIZE * MAX_LEN))\n",
    "training_count = highest_factor * BATCH_SIZE * MAX_LEN\n",
    "print(f'Highest Factor: {highest_factor:,} Training Count: {training_count:,}')\n",
    "\n",
    "# Join the previously tokenized data for tf.keras.preprocessing.text.Tokenizer to work with\n",
    "documents = []\n",
    "for body in posts_df[0:training_count]['_Body'].values.tolist():\n",
    "    words = body.tolist()\n",
    "    documents.append(' '.join(words))\n",
    "\n",
    "labels = labels[0:training_count]\n",
    "\n",
    "# Conserve RAM\n",
    "del posts_df\n",
    "gc.collect()\n",
    "\n",
    "# Verify that lengths for x and y match\n",
    "assert( len(documents) == training_count == labels.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim/Pad the Questions to 200 Words\n",
    "\n",
    "The data has already been truncated to 200 words per post but the tokenization using the top 10K words reduces this to below 200 in some documents. If any documents vary from 200 words, the data won't convert properly into a numpy matrix below.\n",
    "\n",
    "Note that the string `__PAD__` has been used previously to pad the documents, so we reuse it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=TOKEN_COUNT,\n",
    "    oov_token='__PAD__'\n",
    ")\n",
    "tokenizer.fit_on_texts(documents)\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "\n",
    "padded_sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_LEN,\n",
    "    dtype='int32',\n",
    "    padding='post',\n",
    "    truncating='post',\n",
    "    value=1\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del documents\n",
    "del sequences\n",
    "gc.collect()\n",
    "\n",
    "assert( min([len(x) for x in padded_sequences]) == MAX_LEN == max([len(x) for x in padded_sequences]) )\n",
    "\n",
    "# Verify that the padded sequences is a single two dimensional array\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Test/Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences,\n",
    "    labels,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "# Conserve RAM\n",
    "del padded_sequences\n",
    "del labels\n",
    "gc.collect()\n",
    "\n",
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_train.shape[1] == MAX_LEN)\n",
    "assert(X_test.shape[0] == y_test.shape[0]) \n",
    "assert(X_test.shape[1] == MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Class Weights to Balance Uneven Label Counts\n",
    "\n",
    "Although there has already been filtering and up-sampling of the data to restrict it to a sample of questions with at least one tag that occurs more than 2,000 times, there are still uneven ratios between common and uncommon labels. Without class weights, the most common label will be much more likely to be predicted than the least common. Class weights will make the loss function consider uncommon classes more than frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weight_vec = list(np.max(np.sum(y_train, axis=0))/np.sum(y_train, axis=0))\n",
    "train_class_weights = {i: train_weight_vec[i] for i in range(y_train.shape[1])}\n",
    "\n",
    "sorted(list(train_class_weights.items()), key=lambda x: x[1])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN Model to Classify Questions to their Corresponding Tags\n",
    "\n",
    "Now we’re ready to train a model to classify/label questions with tag categories. The model is based on [Kim-CNN](https://arxiv.org/abs/1408.5882), a commonly used convolutional neural network for sentence and document classification. We use the functional API and we’ve heavily parametrized the code so as to facilitate experimentation. \n",
    "\n",
    "In Kim-CNN, we start by encoding the sequences using an *Embedding*, followed by a *Dropout* layer to reduce overfitting. Next we split the graph into multiple *Conv1D* layers with different widths, each followed by *MaxPool1D*. These are joined by concatenation and are intended to characterize patterns of different size sequence lengths in the documents. There follows another *Conv1D*/*GlobalMaxPool1D* layer to summarize the most important of these patterns. This is followed by flattening into a *Dense* layer and then on to the final *sigmoid* output layer. Otherwise we use *selu* throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Activation, Embedding, Flatten, MaxPool1D, GlobalMaxPool1D, Dropout, Conv1D, Input, concatenate\n",
    ")\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "FILTER_LENGTH       = 300\n",
    "FILTER_COUNT        = 128\n",
    "FILTER_SIZES        = [3, 4, 5]\n",
    "EPOCHS              = 8\n",
    "ACTIVATION          = 'selu'\n",
    "CONV_PADDING        = 'same'\n",
    "EMBED_SIZE          = 50\n",
    "EMBED_DROPOUT_RATIO = 0.1\n",
    "CONV_DROPOUT_RATIO  = 0.1\n",
    "LOSS                = 'binary_crossentropy'\n",
    "OPTIMIZER           = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile and fit our model. We use a variety of metrics, because no one metric summarizes model performance, and we need to drill down into the true and false positives and negatives. We also use the *ReduceLROnPlateau*, *EarlyStopping* and *ModelCheckpoint* callbacks to improve performance once we hit a plateau, then to stop early, and to persist only the very best model in terms of the validation categorical accuracy. \n",
    "\n",
    "Categorical accuracy is the best fit for gauging our model’s performance because it gives points for each row separately for each class we’re classifying. This means that if we miss one, but get the others right, this is a great result. With binary accuracy, the entire row is scored as incorrect.\n",
    "\n",
    "We give the fit method the class weights we computed earlier, which really helps model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input = Input(\n",
    "    shape=(X_train.shape[1],), \n",
    "    dtype='int32'\n",
    ")\n",
    "\n",
    "# Create an embedding with RandomUniform initialization\n",
    "emb = Embedding(\n",
    "    TOKEN_COUNT, \n",
    "    EMBED_SIZE, \n",
    "    input_length=X_train.shape[1],\n",
    "    embeddings_initializer=RandomUniform()\n",
    ")(padded_input)\n",
    "drp = Dropout(EMBED_DROPOUT_RATIO)(emb)\n",
    "\n",
    "# Create convlution/max pools using different kernel sizes to summarize features of different \n",
    "# word countss in the document\n",
    "convs = []\n",
    "for filter_size in FILTER_SIZES:\n",
    "    f_conv = Conv1D(\n",
    "        filters=FILTER_COUNT,\n",
    "        kernel_size=filter_size,\n",
    "        padding=CONV_PADDING,\n",
    "        activation=ACTIVATION\n",
    "    )(drp)\n",
    "    f_pool = MaxPool1D()(f_conv)\n",
    "    convs.append(f_pool)\n",
    "\n",
    "l_merge = concatenate(convs, axis=1)\n",
    "l_conv = Conv1D(\n",
    "    128,\n",
    "    5,\n",
    "    activation=ACTIVATION\n",
    ")(l_merge)\n",
    "l_pool = GlobalMaxPool1D()(l_conv)\n",
    "l_flat = Flatten()(l_pool)\n",
    "l_drop = Dropout(CONV_DROPOUT_RATIO)(l_flat)\n",
    "l_dense = Dense(\n",
    "    128,\n",
    "    activation=ACTIVATION\n",
    ")(l_drop)\n",
    "out_dense = Dense(\n",
    "    y_train.shape[1],\n",
    "    activation='sigmoid'\n",
    ")(l_dense)\n",
    "\n",
    "model = Model(inputs=padded_input, outputs=out_dense)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=OPTIMIZER,\n",
    "    loss=LOSS,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.Accuracy(),\n",
    "        tf.keras.metrics.TruePositives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "    ), \n",
    "    EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        patience=2,\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "        filepath='kim_cnn_tagger.weights.hdf5',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    class_weight=train_class_weights,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# Conserve RAM\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Best Model from Training Epochs\n",
    "\n",
    "Because we used `ModelCheckpoint(save_only_best=True)`, the best epoch in terms of `CategoricalAccuracy` is what was saved. We want to use that instead of the last epoch's model, which is what is stored in `model` above. So we load the file before evaluating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('kim_cnn_tagger.weights.hdf5')\n",
    "metrics = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and Print Final Metrics\n",
    "\n",
    "Metrics include names like *precision_66* which aren't consistent between runs. We fix these to cleanup our report on training the model. We also add an f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_metric_name(name):\n",
    "    \"\"\"Remove the trailing _NN, ex. precision_86\"\"\"\n",
    "    if name[-1].isdigit():\n",
    "        repeat_name = '_'.join(name.split('_')[:-1])\n",
    "    else:\n",
    "        repeat_name = name\n",
    "    return repeat_name\n",
    "\n",
    "def fix_value(val):\n",
    "    \"\"\"Convert from numpy to float\"\"\"\n",
    "    return val.item() if isinstance(val, np.float32) else val\n",
    "\n",
    "def fix_metric(name, val):\n",
    "    repeat_name = fix_metric_name(name)\n",
    "    py_val = fix_value(val)\n",
    "    return repeat_name, py_val\n",
    "\n",
    "log = {}\n",
    "for name, val in zip(model.metrics_names, metrics):\n",
    "    repeat_name, py_val = fix_metric(name, val)\n",
    "    log[repeat_name] = py_val\n",
    "\n",
    "log.update({'f1': (log['precision'] * log['recall']) / (log['precision'] + log['recall'])})\n",
    "\n",
    "pd.DataFrame([log])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Epoch Accuracy\n",
    "\n",
    "We want to know the performance at each epoch so that we don't train needlessly large numbers of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "new_history = {}\n",
    "for key, metrics in history.history.items():\n",
    "    new_history[fix_metric_name(key)] = metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "viz_keys = ['val_categorical_accuracy', 'val_precision', 'val_recall']\n",
    "# summarize history for accuracy\n",
    "for key in viz_keys:\n",
    "    plt.plot(new_history[key])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(viz_keys, loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Actual Prediction Outputs\n",
    "\n",
    "It is not enough to know theoretical performance. We need to see the actual output of the tagger at different confidence thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_COUNT = 1000\n",
    "\n",
    "X_test_text = tokenizer.sequences_to_texts(X_test[:TEST_COUNT])\n",
    "\n",
    "y_test_tags = []\n",
    "for row in y_test[:TEST_COUNT].tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col == 1]\n",
    "    y_test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_THRESHOLD = 0.5\n",
    "\n",
    "y_pred = model.predict(X_test[:TEST_COUNT])\n",
    "y_pred = (y_pred > CLASSIFY_THRESHOLD) * 1\n",
    "\n",
    "y_pred_tags = []\n",
    "for row in y_pred.tolist():\n",
    "    tags = [index_tag[str(i)] for i, col in enumerate(row) if col > CLASSIFY_THRESHOLD]\n",
    "    y_pred_tags.append(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Prediction Results\n",
    "\n",
    "It is better to view the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tests = []\n",
    "for x, y, z in zip(X_test_text, y_pred_tags, y_test_tags):\n",
    "    prediction_tests.append({\n",
    "        'Question': x,\n",
    "        'Actual': ' '.join(sorted(z)),\n",
    "        'Predictions': ' '.join(sorted(y)),\n",
    "    })\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(prediction_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Finish\n",
    "\n",
    "That is the big finish! Hopefully you've gotten a sense how to train neural networks using JupyterLab via the DC/OS Data Science Engine.\n",
    "\n",
    "If you like my writing, check out *Weakly Supervised Learning* (O'Reilly, 2020)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
